{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87453acb-d8b3-4e44-8574-2f9c125e2b75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col,xxhash64\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class DataValidator:\n",
    "    def validate_dataframe(self, df: DataFrame, primarykeys: list):\n",
    "        \"\"\"Check if required columns exist, check for null values, and check for unique primary key values for a composite key.\"\"\"\n",
    "    \n",
    "        # Check if each primary key column exists\n",
    "        for primarykey in primarykeys:\n",
    "            if primarykey not in df.columns:\n",
    "                print(f\"Validation failed: Primary key column '{primarykey}' is missing from the DataFrame.\")\n",
    "                raise ValueError(f\"Primary key column '{primarykey}' is missing from the DataFrame.\")\n",
    "            else:\n",
    "                print(f\"Primary key column '{primarykey}' found in DataFrame.\")\n",
    "    \n",
    "            # Check for null values in the primary key column\n",
    "            null_count = df.filter(df[primarykey].isNull()).count()\n",
    "            if null_count > 0:\n",
    "                print(f\"Validation failed: Primary key column '{primarykey}' contains {null_count} null values.\")\n",
    "                raise ValueError(f\"Primary key column '{primarykey}' contains null values.\")\n",
    "            else:\n",
    "                print(f\"No null values found in primary key column '{primarykey}'.\")\n",
    "        \n",
    "        # Check for uniqueness of the composite primary key (combination of the primary key columns)\n",
    "        distinct_count = df.select(primarykeys).distinct().count()\n",
    "        total_count = df.count()\n",
    "        if distinct_count != total_count:\n",
    "            print(f\"Validation failed: Composite primary key contains {total_count - distinct_count} duplicate rows.\")\n",
    "            raise ValueError(f\"Composite primary key contains duplicate rows.\")\n",
    "        else:\n",
    "            print(f\"Composite primary key contains unique rows.\")\n",
    "        \n",
    "        # If all checks pass\n",
    "        print(\"DataFrame validation passed successfully.\")\n",
    "        return \n",
    "\n",
    "    \n",
    "    \n",
    "    def DuplicateTable(self, df: DataFrame, s_table_name: str) -> None:\n",
    "        \n",
    "        # Load the existing silver table DataFrame\n",
    "        s_df = spark.table(s_table_name)\n",
    "\n",
    "        \n",
    "        metadata_cols = [\"start_date\", \"updated_date\"]\n",
    "        dup_check_cols = [x for x in s_df.columns if x not in set(metadata_cols)]\n",
    "        dup_df = df.join(s_df, dup_check_cols, 'inner').select(df.columns)\n",
    "        \n",
    "        \n",
    "        # Debugging: Print the count of rows in both DataFrames\n",
    "        print(f\"Row count in new DataFrame: {dup_df.count()}\")\n",
    "        print(f\"Row count in existing DataFrame: {s_df.count()}\")\n",
    "        source_df = df.subtract(dup_df)\n",
    "        # If the number of matching rows equals the number of rows in df, print and raise an error\n",
    "        if source_df.count() == 0 : \n",
    "            print(\"The new DataFrame contains duplicate entries from the existing table.\")\n",
    "            raise ValueError(\"The new DataFrame contains duplicate entries from the existing table.\")\n",
    "        else:\n",
    "            print(\"The new DataFrame does not contain duplicate entries from the existing table.\")\n",
    "\n",
    "        return True  # Return True if they are not identical\n",
    "    \n",
    "    def get_table_schema(self,table_name):\n",
    "        \"\"\"Fetch schema (column names and types) from an existing table using `describe`.\"\"\"\n",
    "        schema_df = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "        table_schema ={row['col_name']: row['data_type'] for row in schema_df.collect()}\n",
    "        return table_schema\n",
    "\n",
    "    def get_df_schema(self,df):\n",
    "        \"\"\"Get schema (column names and types) from a DataFrame.\"\"\"\n",
    "        dfschema = dict(df.dtypes)\n",
    "        return dfschema\n",
    "\n",
    "    def test_schema_match(self,table_schema, df_schema):\n",
    "       \n",
    "        # Check that each column in the DataFrame matches the corresponding column in the table\n",
    "        for col_name, df_dtype in df_schema.items():\n",
    "            if col_name in table_schema:\n",
    "                # Assert the data type matches if column names match\n",
    "                assert table_schema[col_name] == df_dtype, (\n",
    "                    f\"Data type mismatch for column '{col_name}'. \"\n",
    "                    f\"Table data type: {table_schema[col_name]}, DataFrame data type: {df_dtype}\"\n",
    "                )\n",
    "            else:\n",
    "                # Raise an assertion error if a column in the DataFrame is not found in the table schema\n",
    "                pytest.fail(f\"Column '{col_name}' not found in the table schema.\")\n",
    "    \n",
    "        print(f\"Schema verification successful: All columns and data types match for table .\")\n",
    "    \n",
    "    def runschema(self,table_name, df):\n",
    "        validator = DataValidator()\n",
    "        tabl_sch= validator.get_table_schema(table_name)\n",
    "        df_sch = validator.get_df_schema(df)\n",
    "        validator.test_schema_match(tabl_sch,df_sch)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pytest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
